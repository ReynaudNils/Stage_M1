library(zipfR)
ZM = lnre("zm", alpha = 1.5, B = 0.1)
zmsample = rlnre(ZM, n=100)
zmnumeric = as.numeric(as.character(zmsample))
plot(zmnumeric)
library(zipfR)
ZM = lnre("zm", alpha = 1.5, B = 0.1)
zmsample = rlnre(ZM, n=1000)
zmnumeric = as.numeric(as.character(zmsample))
plot(zmnumeric)
zmsample = rlnre(ZM, n=500)
library(zipfR)
ZM = lnre("zm", alpha = 1.5, B = 0.1)
zmsample = rlnre(ZM, n=500)
zmnumeric = as.numeric(as.character(zmsample))
plot(zmnumeric)
library(zipfR)
ZM = lnre("zm", alpha = 1.5, B = 0.1)
zmsample = rlnre(ZM, n=100)
zmnumeric = as.numeric(as.character(zmsample))
plot(zmnumeric)
library(ggplot2)
library(dplyr)
library(themes)
library(gganimate)
word_count <- # Data frame containing words and their frequency
colnames(word_count) <- c("word", "count")
alpha <- 1 # Change it needed
word_count <- word_count %>%
mutate(word = factor(word, levels = word),
rank = row_number(),
zipfs_freq = ifelse(rank == 1, count, dplyr::first(count) / rank^alpha))
zipfs_plot <- ggplot(word_count, aes(x = rank, y = count)) +
geom_point(aes(color = "observed")) +
theme_bw() +
geom_point(aes(y = zipfs_freq, color = "theoretical")) +
transition_reveal(count, rank) +
labs(x = "rank", y = "count", title = "Zipf's law visualization") +
scale_colour_manual(name = "Word count", values=c("theoretical" = "red", "observed" = "black")) +
theme(legend.position = "top")
zipfs_animation <- animate(p)
data(BCI)
data(meaudret)
vegan
library(vegan)
plot(1:10810)
source("C:/Users/nilsr/Desktop/PRE/ZM-Fitting.R")
help(mle)
??mle
getwd()
setwd('C:/Users/nilsr/Bureau/PRE/Stage_M1/Donnees_CSV')
#plot(Genera_Sedum$Images_Sedum, log="xy")
#title(main = "Plot avec q=O")
#plot(Genera_Sedum$Rank +0.5, Genera_Sedum$Images_Sedum, log="xy", col="blue")
#plot(Genera_Sedum$Rank +5, Genera_Sedum$Images_Sedum, log="xy", col="red")
#plot(Genera_Sedum$Rank +10, Genera_Sedum$Images_Sedum, log="xy", col="green")
#plot(Genera_Sedum$Rank +50, Genera_Sedum$Images_Sedum, log="xy", col="brown")
#plot(Genera_Sedum$Rank +100, Genera_Sedum$Images_Sedum, log="xy", col="purple")
#plot(Genera_Sedum$Rank +mean(q.hats), Genera_Sedum$Images_Sedum, log="xy")
#title(main="Plot avec le q optimisé")
#plot(x, 8.69x)
setwd('C:/Users/nilsr/Bureau/PRE/Stage_M1/Donnees_CSV')
#plot(Genera_Sedum$Images_Sedum, log="xy")
#title(main = "Plot avec q=O")
#plot(Genera_Sedum$Rank +0.5, Genera_Sedum$Images_Sedum, log="xy", col="blue")
#plot(Genera_Sedum$Rank +5, Genera_Sedum$Images_Sedum, log="xy", col="red")
#plot(Genera_Sedum$Rank +10, Genera_Sedum$Images_Sedum, log="xy", col="green")
#plot(Genera_Sedum$Rank +50, Genera_Sedum$Images_Sedum, log="xy", col="brown")
#plot(Genera_Sedum$Rank +100, Genera_Sedum$Images_Sedum, log="xy", col="purple")
#plot(Genera_Sedum$Rank +mean(q.hats), Genera_Sedum$Images_Sedum, log="xy")
#title(main="Plot avec le q optimisé")
#plot(x, 8.69x)
setwd('C:/Users/nilsr/Desktop/PRE/Stage_M1/Donnees_CSV')
N <- 59    # the size of my dataset
x <- read.csv("Especes_Sedum.csv")
x=Genera_Sedum$Images_Sedum
N <- 59    # the size of my dataset
Genera_Sedum <- read.csv("Especes_Sedum.csv")    # my dataset file
x=Genera_Sedum$Images_Sedum
negloglik <- function(parms1, parms2) {
invC <- sum(1/(1:N + parms1)^parms2)
return(-sum(log(1/((x + parms1)^(parms2)*invC))))
}
library(stats4)
negloglik(2, 4)
mle(negloglik, start=list(parms1=20, parms2=20), method = )
setwd('C:/Users/nilsr/Bureau/PRE/Stage_M1/Donnees_CSV')
setwd('C:/Users/nilsr/Desktop/PRE/Stage_M1/Donnees_CSV')
N <- 59    # the size of my dataset
Genera_Sedum <- read.csv("Especes_Sedum.csv")    # my dataset file
x=Genera_Sedum$Images_Sedum
negloglik <- function(parms1, parms2) {
invC <- sum(1/(1:N + parms1)^parms2)
return(-sum(log(1/((x + parms1)^(parms2)*invC))))
}
library(stats4)
negloglik(2, 4)
#mle(negloglik, start=list(parms1=20, parms2=20), method = )
#negloglik(c(1,1), x=2 )
plot(d <- sapply(1:N, function(x) exp(-negloglik(5, 5))))
help(optim)
mle(negloglik, start=list(parms1=20, parms2=20), method = Nelder-Mead)
mle(negloglik, start=list(parms1=20, parms2=20), method = "Nelder-Mead")
#Essayons différentes méthodes d'optimisation
#D'abord la méthode par défaut, Nelder-Mead, qui ne donne rien
mle(negloglik, start=list(parms1=2, parms2=4), method = "Nelder-Mead")
#Ensuite BFGS
mle(negloglik, start=list(parms1=2, parms2=4), method = "BFGS")
#
mle(negloglik, start=list(parms1=2, parms2=4), method = "CG")
mle(negloglik, start=list(parms1=2, parms2=4), method = "L-BFGS-B")
mle(negloglik, start=list(parms1=2, parms2=4), method = "SANN")
mle(negloglik, start=list(parms1=2, parms2=4), method = "Brent")
Sann <- mle(negloglik, start=list(parms1=2, parms2=4), method = "SANN")
#La méthode SANN elle fonctionne
Sann <- mle(negloglik, start=list(parms1=2, parms2=4), method = "SANN")
#La méthode SANN elle fonctionne
mle(negloglik, start=list(parms1=2, parms2=4), method = "SANN")
#La méthode SANN elle fonctionne
mle(negloglik, start=list(parms1=2, parms2=4), method = "SANN")
#La méthode SANN elle fonctionne
mle(negloglik, start=list(parms1=2, parms2=4), method = "SANN", hessian = FALSE)
gradient <- function(parms1, parms2) {
invC <- sum(1/(1:N + parms1)^parms2)
return(-sum((parms2/(x + parms1)^(parms2 + 1)*invC)))
}
mle(negloglik, start=list(parms1=2, parms2=4), gr = gradient, method = "Nelder-Mead")
gradient(2, 4)
mle(negloglik, start=list(parms1=2, parms2=4), gr = gradient, method = "BFGS")
help(gr)
??gr
mle(negloglik, par=list(parms1=2, parms2=4), gr = gradient, method = "BFGS")
mle(fn=negloglik, par=list(parms1=2, parms2=4), gr = gradient, method = "BFGS")
mle(minuslogl = negloglik, par=list(parms1=2, parms2=4), gr = gradient, method = "BFGS")
#Essayons différentes méthodes d'optimisation
#D'abord la méthode par défaut, Nelder-Mead, qui ne donne rien
mle(negloglik, start=list(parms1=2, parms2=4), method = "Nelder-Mead")
#Ensuite d'autres méthodes ne fonctionnant pas
mle(negloglik, start=list(parms1=2, parms2=4), method = "BFGS")
mle(negloglik, start=list(parms1=2, parms2=4), method = "CG")
mle(negloglik, start=list(parms1=2, parms2=4), method = "L-BFGS-B")
#La méthode SANN elle fonctionne
mle(negloglik, start=list(parms1=2, parms2=4), method = "SANN", hessian = FALSE)
#La méthode SANN elle fonctionne
mle(negloglik, start=list(parms1=2, parms2=4), method = "SANN")
#La méthode SANN elle fonctionne
mle(negloglik, start=list(parms1=2, parms2=4), gr = gradient, method = "SANN")
#La méthode SANN elle fonctionne
mle(negloglik, start=list(parms1=2, parms2=4), gr = gradient(parms1=2, parms2=4), method = "SANN")
#mle(minuslogl = negloglik, par=list(parms1=2, parms2=4), gr = gradient, method = "BFGS")
optim(par=list(parms1=2, parms2=4), fn=negloklik, gr=gradient)
#mle(minuslogl = negloglik, par=list(parms1=2, parms2=4), gr = gradient, method = "BFGS")
optim(par=list(parms1=2, parms2=4), negloklik, gr=gradient)
#mle(minuslogl = negloglik, par=list(parms1=2, parms2=4), gr = gradient, method = "BFGS")
optim(list(parms1=2, parms2=4), negloklik, gr=gradient)
#mle(minuslogl = negloglik, par=list(parms1=2, parms2=4), gr = gradient, method = "BFGS")
optim(negloklik, start=list(parms1=2, parms2=4), gr=gradient)
help(nlm)
nlm(negloklik, start=list(parms1=2, parms2=4), gradtol=1e-9, steptol=1e-9)
nlm(negloklik, p=c(parms1=2, parms2=4), gradtol=1e-9, steptol=1e-9)
nlm(function(p) negloklik(p), p=c(2, 4), gradtol=1e-9, steptol=1e-9)
negloglik <- function(parms1, parms2) {
invC <- sum(1/(1:N + parms1)^parms2)
return(-sum(log(1/((x + parms1)^(parms2)*invC))))
}
nlm(function(p) negloklik(p), p=c(2, 4), gradtol=1e-9, steptol=1e-9)
negloglik <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
return(-sum(log(1/((x + parms[1])^(parms[2])*invC))))
}
nlm(function(p) negloklik(p), p=c(2, 4), gradtol=1e-9, steptol=1e-9)
nlm(function(p) negloglik(p), p=c(2, 4), gradtol=1e-9, steptol=1e-9)
warnings()
#Let's use different geenral optimizers of R
import::from("rbenchmark", "benchmark")
#Let's use different geenral optimizers of R
library(rbenchmark)
devtools::install_github("eddelbuettel/rbenchmark")
#Let's use different geenral optimizers of R
optim = optim(par = rep(2, 4), fn = negloglik, hessian = F)
nlm = nlm(f = negloglik, p = rep(2, 4), hessian = F)
warnings()
nlminb = nlminb(start = rep(2, 4), objective = negloglik)
warnings()
gradient <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
return(-sum((parms[2]/(x + parms{1})^(parms[2] + 1)*invC)))
gradient <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
return(-sum((parms[2]/(x + parms[1])^(parms[2] + 1)*invC)))
}
nlminb = nlminb(start = rep(2, 4), objective = negloglik, gradient = gradient)
help(rep)
#Let's use different geenral optimizers of R
optim = optim(par = rep[2, 4], fn = negloglik, hessian = F)
negloglik <- function(parm1, parm2) {
invC <- sum(1/(1:N + parm1)^parm2)
return(-sum(log(1/((x + parm1)^(parm2)*invC))))
}
#Let's use different geenral optimizers of R
optim = optim(par = rep(2, 4), fn = negloglik, hessian = F)
negloglik <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
return(-sum(log(1/((x + parms[1])^(parms[2])*invC))))
}
#Let's use different geenral optimizers of R
optim = optim(par = rep(2, 4), fn = negloglik, hessian = F)
nlm = nlm(f = negloglik, p = rep(2, 4), hessian = F)
nlminb = nlminb(start = rep(2, 4), objective = negloglik, gradient = gradient)
optim
nlm
mle(negloglik, start=list(parms[1]=2, parms[2]=4))
mle(negloglik, start=list(parms1=2, parms2=4))
mle(negloglik, start=list(2, 4))
#Essayons différentes méthodes d'optimisation
#D'abord la méthode par défaut, Nelder-Mead, qui ne donne rien
mle(negloglik, start=list(parms1=2, parms2=4), method = "Nelder-Mead")
mle(negloglik, start=c(2, 4))
mle(negloglik, start=[2, 4])
negloglik2 <- function(parm1, parm2) {
invC <- sum(1/(1:N + parm1)^parm2)
return(-sum(log(1/((x + parm1)^(parm2)*invC))))
}
mle(negloglik2, start=list(2, 4))
negloglik2 <- function(parms1, parms2) {
invC <- sum(1/(1:N + parms1)^parms2)
return(-sum(log(1/((x + parms1)^(parms2)*invC))))
}
mle(negloglik2, start=list(parms1=2, parms2=4))
optim
nlminb = nlminb(start = rep(2, 4), objective = negloglik)
library(ucminf)
install.packages("ucminf")
#Let's use different geenral optimizers of R
optim = optim(par = rep(2, 4), fn = negloglik, hessian = F)
library(ucminf)
ucminf::ucminf(par=rep(2, 4), fn = negloglik, hessian = 0)
ucminf::ucminf(par=rep(2, 4), fn = negloglik, hessian = 1)
install.packages("numDeriv")
library(numDeriv)
ucminf::ucminf(par=rep(2, 4), fn = negloglik, hessian = 1)
#Let's use different geenral optimizers of R
optim = optim(par = rep(2, 4), fn = negloglik, hessian = F)
optim
mle = mle(negloglik2, start=list(parms1=2, parms2=4))
mle
ml = mle(negloglik2, start=list(parms1=2, parms2=4))
ml
#Let's use different geenral optimizers of R
optim = optim(par = rep(2, 4), fn = negloglik)
optim
help(optim)
mle(negloglik, start=list(parms1=2, parms2=4), gr = gradient(parms1=2, parms2=4), method = "SANN")
mle(negloglik2, start=list(parms1=2, parms2=4), gr = gradient(parms1=2, parms2=4), method = "SANN")
mle(negloglik2, start=list(parms1=2, parms2=4), gr = gradient, method = "SANN")
N <- 303    # the size of my dataset
Genera_Sedum <- read.csv("Especes2.csv")    # my dataset file
x=Genera_Sedum$Images_Sedum
negloglik <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
return(-sum(log(1/((x + parms[1])^(parms[2])*invC))))
}
negloglik2 <- function(parms1, parms2) {
invC <- sum(1/(1:N + parms1)^parms2)
return(-sum(log(1/((x + parms1)^(parms2)*invC))))
}
gradient <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
return(-sum((parms[2]/(x + parms[1])^(parms[2] + 1)*invC)))
}
negloglik(2, 4)
negloglik2(2, 4)
gradient(2, 4)
gradient(c(2, 4))
#Let's use different geenral optimizers of R
optim = optim(par = rep(2, 4), fn = negloglik, hessian = F)
optim
nlm
#Let's use different geenral optimizers of R
optim = optim(par = rep(2, 4), fn = negloglik, hessian = F)
nlm = nlm(f = negloglik, p = rep(2, 4), hessian = F)
optim
nlm
N <- 59    # the size of my dataset
Genera_Sedum <- read.csv("Especes_Sedum.csv")    # my dataset file
x=Genera_Sedum$Images_Sedum
negloglik <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
return(-sum(log(1/((x + parms[1])^(parms[2])*invC))))
}
negloglik2 <- function(parms1, parms2) {
invC <- sum(1/(1:N + parms1)^parms2)
return(-sum(log(1/((x + parms1)^(parms2)*invC))))
}
gradient <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
return(-sum((parms[2]/(x + parms[1])^(parms[2] + 1)*invC)))
}
library(stats4)
negloglik2(2, 4)
gradient(c(2, 4))
#Essayons différentes méthodes d'optimisation
#D'abord la méthode par défaut, Nelder-Mead, qui ne donne rien
mle(negloglik, start=list(parms1=2, parms2=4), method = "Nelder-Mead")
#Essayons différentes méthodes d'optimisation
#D'abord la méthode par défaut, Nelder-Mead, qui ne donne rien
mle(negloglik2, start=list(parms1=2, parms2=4), method = "Nelder-Mead")
#Let's use different geenral optimizers of R
optim = optim(par = rep(2, 4), fn = negloglik, hessian = F)
nlm = nlm(f = negloglik, p = rep(2, 4), hessian = F)
ml = mle(negloglik2, start=list(parms1=2, parms2=4))
optim
nlm
gradient <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
A <- sum(parms[2]/x + parms[1])
B <- sum(log(x + parms[1]))
return(c(A, B))
}
#Let's use different geenral optimizers of R
optim = optim(par = rep(2, 4), fn = negloglik, gr = gradient, hessian = F)
optim
optim = optim(par = rep(2, 4), fn = negloglik, hessian = F)
optim2 = optim(par = rep(2, 4), fn = negloglik, gr = gradient, hessian = F)
optim
optim2
help(nlm)
nlm2 = nlm(f = negloglik, p = rep(2, 4), gr = gradient, hessian = F)
negloglik3 <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
res <- -sum(log(1/((x + parms[1])^(parms[2])*invC)))
A <- sum(parms[2]/x + parms[1])
B <- sum(log(x + parms[1]))
attr(res, "gradient") <- c(A, B)
res
}
nlm2 = nlm(f = negloglik, p = rep(2, 4), hessian = F)
warnings()
nlm2
nlm = nlm(f = negloglik, p = rep(2, 4), hessian = F)
nlm
optim
optim2
nlm
nlm = nlm(f = negloglik3, p = rep(2, 4), hessian = F)
warnings()
nlm
help(mle)
ml = mle(negloglik2, start=list(parms1=2, parms2=4))
ml = mle(negloglik, start=list(parms1=2, parms2=4))
ml = mle(negloglik3, start=list(parms1=2, parms2=4))
negloglik4 <- function(parms1, parms2) {
invC <- sum(1/(1:N + parms1)^parms2)
res <- -sum(log(1/((x + parms1)^(parms2)*invC)))
A <- sum(parms2/x + parms1)
B <- sum(log(x + parms1))
attr(res, "gradient") <- c(A, B)
res
}
ml = mle(negloglik4, start=list(parms1=2, parms2=4))
#Essayons différentes méthodes d'optimisation
#D'abord la méthode par défaut, Nelder-Mead, qui ne donne rien
mle(negloglik4, start=list(parms1=2, parms2=4), method = "Nelder-Mead")
#Ensuite d'autres méthodes ne fonctionnant pas
mle(negloglik4, start=list(parms1=2, parms2=4), method = "BFGS")
mle(negloglik4, start=list(parms1=2, parms2=4), method = "CG")
mle(negloglik4, start=list(parms1=2, parms2=4), method = "L-BFGS-B")
mle(negloglik4, start=list(parms1=2, parms2=4), gr = gradient, method = "SANN")
ml <- mle(negloglik4, start=list(parms1=2, parms2=4), gr = gradient, method = "SANN")
ml
nlminb = nlminb(start = rep(2, 4), objective = negloglik)
nlminb = nlminb(start = rep(2, 4), objective = negloglik3)
H <- matrix(1:9, nrow=3, byrow=TRUE)
H
H <- matrix(1:4, nrow=2, byrow=TRUE)
H
H <- matrix(1:4, nrow=2, byrow=TRUE)
H[1,1] <- -sum(parms[2]/(x + parms[1])^2)
negloglik5 <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
res <- -sum(log(1/((x + parms[1])^(parms[2])*invC)))
A <- sum(parms[2]/x + parms[1])
B <- sum(log(x + parms[1]))
H <- matrix(1:4, nrow=2, byrow=TRUE)
H[1,1] <- -sum(parms[2]/(x + parms[1])^2)
H[2,2] <- 0
H[1,2] <- sum(1/(x + parms[1]))
H[2,1] <- sum(1/(x + parms[1]))
attr(res, "gradient") <- c(A, B)
attr(res, "hessian") <- H
res
}
help(optim)
help(nlm)
nlm2 = nlm(f = negloglik5, p = rep(2, 4), hessian = True)
nlm2 = nlm(f = negloglik5, p = rep(2, 4), hessian = TRUE)
warnings()
nlm2
negloglik6 <- function(parms1, parms2) {
invC <- sum(1/(1:N + parms1)^parms2)
res <- -sum(log(1/((x + parms1)^(parms2)*invC)))
A <- sum(parms2/x + parms1)
B <- sum(log(x + parms1))
H <- matrix(1:4, nrow=2, byrow=TRUE)
H[1,1] <- -sum(parms2/(x + parms1)^2)
H[2,2] <- 0
H[1,2] <- sum(1/(x + parms1))
H[2,1] <- sum(1/(x + parms1))
attr(res, "gradient") <- c(A, B)
attr(res, "hessian") <- H
res
}
#Essayons différentes méthodes d'optimisation
#D'abord la méthode par défaut, Nelder-Mead, qui ne donne rien
mle(negloglik6, start=list(parms1=2, parms2=4), method = "Nelder-Mead")
#Ensuite d'autres méthodes ne fonctionnant pas
mle(negloglik6, start=list(parms1=2, parms2=4), method = "BFGS")
mle(negloglik6, start=list(parms1=2, parms2=4), method = "CG")
mle(negloglik6, start=list(parms1=2, parms2=4), method = "L-BFGS-B")
ml <- mle(negloglik6, start=list(parms1=2, parms2=4), gr = gradient, method = "SANN")
ml
help(mle)
optim2 = optim(par = rep(2, 4), fn = negloglik, gr = gradient, hessian = T)
optim2 = optim(par = rep(2, 4), fn = negloglik, gr = gradient, hessian = F)
optim2
help(optim)
nlm = nlm(f = negloglik3, p = rep(2, 4), hessian = F)
warnings()
help(nlm)
optim2 = optim(par = rep(2, 4), fn = negloglik, gr = gradient, hessian = F)
optim2
help(optim)
optim2 = optim(par = rep(2, 4), fn = negloglik, gr = gradient, hessian = F, method="CG")
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, hessian = F, method="CG")
optim2
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, hessian = F, method="LBFGS")
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, hessian = F, method="BFGS")
optim2
#Let's use different geenral optimizers of R
optim = optim(par = c(2, 4), fn = negloglik, hessian = F)
optim
gradient(optim$par)
gradient(optim2$par)
gradient <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
tampon <- sum(-parms[2]/(1:N + parms[1])^(parms[2]+1))
tampon2 <- sum(exp(-parms[2]*log(1:N + parms[1])))
tampon3 <- sum(-log(1:N + parms[1])*exp(-parms[2]*log(1:N + parms[1])))
A <- sum((parms[2]/x + parms[1])) + (tampon/invC)
B <- sum(log(x + parms[1])) + (tampon3/tampon2)
return(c(A, B))
}
#Let's use different geenral optimizers of R
optim = optim(par = c(2, 4), fn = negloglik, hessian = F)
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, hessian = F, method="BFGS")
optim
optim2
gradient(optim$par)
gradient(optim2$par)
gradient <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
tampon <- sum(-parms[2]/(1:N + parms[1])^(parms[2]+1))
tampon2 <- sum(exp(-parms[2]*log(1:N + parms[1])))
tampon3 <- sum(-log(1:N + parms[1])*exp(-parms[2]*log(1:N + parms[1])))
A <- sum((parms[2]/x + parms[1]) + (tampon/invC))
B <- sum(log(x + parms[1]) + (tampon3/tampon2))
return(c(A, B))
}
#Let's use different geenral optimizers of R
optim = optim(par = c(2, 4), fn = negloglik, hessian = F)
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, hessian = F, method="BFGS")
optim
optim2
gradient(optim2$par)
gradient(optim$par)
gradient <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
tampon <- sum(-parms[2]/(1:N + parms[1])^(parms[2]+1))
tampon2 <- sum(exp(-parms[2]*log(1:N + parms[1])))
tampon3 <- sum(-log(1:N + parms[1])*exp(-parms[2]*log(1:N + parms[1])))
A <- sum((parms[2]/x + parms[1])) + (tampon/invC)
B <- sum(log(x + parms[1])) + (tampon3/tampon2)
return(c(A, B))
}
#Let's use different geenral optimizers of R
optim = optim(par = c(2, 4), fn = negloglik, hessian = F)
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, hessian = F, method="BFGS")
gradient <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
tampon <- sum(-parms[2]/(1:N + parms[1])^(parms[2]+1))
tampon2 <- sum(exp(-parms[2]*log(1:N + parms[1])))
tampon3 <- sum(-log(1:N + parms[1])*exp(-parms[2]*log(1:N + parms[1])))
A <- sum((parms[2]/x + parms[1])) + (tampon/invC)
B <- sum(log(x + parms[1])) + (tampon3/tampon2)
return(c(A, B))
}
gradient <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
tampon <- sum(-parms[2]/(1:N + parms[1])^(parms[2]+1))
tampon2 <- sum(exp(-parms[2]*log(1:N + parms[1])))
tampon3 <- sum(-log(1:N + parms[1])*exp(-parms[2]*log(1:N + parms[1])))
A <- sum((parms[2]/(x + parms[1]))) + (tampon/invC)
B <- sum(log(x + parms[1])) + (tampon3/tampon2)
return(c(A, B))
}
#Let's use different geenral optimizers of R
optim = optim(par = c(2, 4), fn = negloglik, hessian = F)
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, hessian = F, method="BFGS")
optim
optim2
gradient(optim$par)
gradient(optim2$par)
