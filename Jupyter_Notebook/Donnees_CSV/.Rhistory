res
}
On teste les fonctions
#On teste les fonctions
negloglik(parms = c(2, 4))
negloglik2(2, 4)
negloglik3(parms = c(2, 4))
negloglik4(2, 4)
gradient(c(2, 4))
optim = optim(par = c(2, 4), fn = negloglik, hessian = F)
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, hessian = F, method="BFGS")
optim
optim2
gradient(optim$par)
gradient(optim2$par)
nlm <- nlm(negloglik, p=c(20,20), gradtol = 1e-12, steptol = 1e-9)
nlm2 = nlm(f = negloglik3, p = rep(2, 4), hessian = F)
#nlm3 = nlm(f = negloglik5, p = rep(2, 4), hessian = TRUE)
nlm
nlm2
nlm3
#Ensuite nlm, où le gradient doit être précisé comme attribut de la fonction, on utilise donc negloglik3
nlm <- nlm(negloglik, p=c(20,20), gradtol = 1e-12, steptol = 1e-9)
nlm2 = nlm(f = negloglik3, p = c(2, 4), hessian = F)
getwd()
setwd('C:/Users/nilsr/Desktop/PRE/Stage_M1/Jupyter_Notebook/Donnees_CSV')
library(stats4)
N <- 59    # the size of my dataset
Genera_Sedum <- read.csv("Especes_Sedum.csv")    # my dataset file
x=Genera_Sedum$Images_Sedum
#Définissons la fonction maximum de vraisemblance
#D'abord juste le negloglikelihood
negloglik <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
return(-sum(log(1/((x + parms[1])^(parms[2])*invC))))
}
negloglik2 <- function(parms1, parms2) {
invC <- sum(1/(1:N + parms1)^parms2)
return(-sum(log(1/((x + parms1)^(parms2)*invC))))
}
#Ensuite avec le gradient en plus
negloglik3 <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
res <- -sum(log(1/((x + parms[1])^(parms[2])*invC)))
tampon <- sum(-parms[2]/(1:N + parms[1])^(parms[2]+1))
tampon2 <- sum(exp(-parms[2]*log(1:N + parms[1])))
tampon3 <- sum(-log(1:N + parms[1])*exp(-parms[2]*log(1:N + parms[1])))
A <- sum((parms[2]/(x + parms[1]))) + (tampon/invC)
B <- sum(log(x + parms[1])) + (tampon3/tampon2)
attr(res, "gradient") <- c(A, B)
res
}
negloglik4 <- function(parms1, parms2) {
invC <- sum(1/(1:N + parms1)^parms2)
res <- -sum(log(1/((x + parms1)^(parms2)*invC)))
tampon <- sum(-parms2/(1:N + parms1)^(parms2+1))
tampon2 <- sum(exp(-parms2*log(1:N + parms1)))
tampon3 <- sum(-log(1:N + parms1)*exp(-parms2*log(1:N + parms1)))
A <- sum((parms2/(x + parms1))) + (tampon/invC)
B <- sum(log(x + parms1)) + (tampon3/tampon2)
attr(res, "gradient") <- c(A, B)
res
}
#La fonction gradient
gradient <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
tampon <- sum(-parms[2]/(1:N + parms[1])^(parms[2]+1))
tampon2 <- sum(exp(-parms[2]*log(1:N + parms[1])))
tampon3 <- sum(-log(1:N + parms[1])*exp(-parms[2]*log(1:N + parms[1])))
A <- sum((parms[2]/(x + parms[1]))) + (tampon/invC)
B <- sum(log(x + parms[1])) + (tampon3/tampon2)
return(c(A, B))
}
#Enfin avec la hessienne en plus, à coder avec les bons termes
#negloglik5 <- function(parms) {
#  invC <- sum(1/(1:N + parms[1])^parms[2])
#  res <- -sum(log(1/((x + parms[1])^(parms[2])*invC)))
#  tampon <- sum(-parms[2]/(1:N + parms[1])^(parms[2]+1))
#  tampon2 <- sum(exp(-parms[2]*log(1:N + parms[1])))
#  tampon3 <- sum(-log(1:N + parms[1])*exp(-parms[2]*log(1:N + parms[1])))
#  A <- sum((parms[2]/(x + parms[1]))) + (tampon/invC)
#  B <- sum(log(x + parms[1])) + (tampon3/tampon2)
#  H <- matrix(1:4, nrow=2, byrow=TRUE)
#  H[1,1] <- -sum(parms[2]/(x + parms[1])^2)
#  H[2,2] <- 0
#  H[1,2] <- sum(1/(x + parms[1]))
#  H[2,1] <- sum(1/(x + parms[1]))
#  attr(res, "gradient") <- c(A, B)
#  attr(res, "hessian") <- H
#  res
#}
#negloglik6 <- function(parms1, parms2) {
#  invC <- sum(1/(1:N + parms1)^parms2)
#  res <- -sum(log(1/((x + parms1)^(parms2)*invC)))
#  invC <- sum(1/(1:N + parms1)^parms2)
#  tampon <- sum(-parms2/(1:N + parms1)^(parms2+1))
#  tampon2 <- sum(exp(-parms2*log(1:N + parms1)))
#  tampon3 <- sum(-log(1:N + parms1)*exp(-parms2*log(1:N + parms1)))
#  A <- sum((parms2/(x + parms1))) + (tampon/invC)
#  B <- sum(log(x + parms1)) + (tampon3/tampon2)
#  H <- matrix(1:4, nrow=2, byrow=TRUE)
#  H[1,1] <- -sum(parms2/(x + parms1)^2)
#  H[2,2] <- 0
#  H[1,2] <- sum(1/(x + parms1))
#  H[2,1] <- sum(1/(x + parms1))
#  attr(res, "gradient") <- c(A, B)
#  attr(res, "hessian") <- H
#  res
#}
#On teste les fonctions
negloglik(parms = c(2, 4))
negloglik2(2, 4)
negloglik3(parms = c(2, 4))
negloglik4(2, 4)
gradient(c(2, 4))
#Essayons différentes méthodes d'optimisation avec mle
#D'abord la méthode par défaut, Nelder-Mead, qui ne donne rien
mle(negloglik2, start=list(parms1=2, parms2=4), method = "Nelder-Mead")
#Let's use different general optimizers of R
#D'abord optim, où l'on peut préciser le gradient avec gr=...
#d'abord sans préciser le gradient, puis ensuite en le précisant
optim = optim(par = c(2, 4), fn = negloglik, hessian = F)
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, hessian = F, method="BFGS")
optim
optim2
#Vérifions que le gradient est nul aux points optimaux
gradient(optim$par)
gradient(optim2$par) #Ce n'est pas le cas...
#Ensuite nlm, où le gradient doit être précisé comme attribut de la fonction, on utilise donc negloglik3
nlm <- nlm(negloglik, p=c(20,20), gradtol = 1e-12, steptol = 1e-9)
nlm2 = nlm(f = negloglik3, p = c(2, 4), hessian = F)
getwd()
setwd('C:/Users/nilsr/Desktop/PRE/Stage_M1/Jupyter_Notebook/Donnees_CSV')
#On veut modéliser de la même manière pour chaque genre contenant un nombre suffisant d'espèces
#Modélisons le Genre Sedum
Genera_Sedum <- read.csv('Especes_Sedum.csv')
#install.packages("dplyr")
#desc <- dplyr::desc
#library(dplyr)
Genera_Acacia <- read.csv('Especes_Acacia.csv')
Genera_Trifolium <- read.csv('Especes_Trifolium.csv')
Genera_Hypericum <- read.csv('Especes_Hypericum.csv')
Genera_Ophrys <- read.csv('Especes_Ophrys.csv')
Genera_Anemone <- read.csv('Especes_Anemone.csv')
Genera_Cirsium <- read.csv('Especes_Cirsium.csv')
Genera_Pelargonium <- read.csv('Especes_Pelargonium.csv')
Genera_Peperomia <- read.csv('Especes_Peperomia.csv')
Genera_Lupinus <- read.csv('Especes_Lupinus.csv')
Genera_Dryopteris <- read.csv('Especes_Dryopteris.csv')
Genera_Lactuca <- read.csv('Especes_Lactuca.csv')
Genera_Tradescantia <- read.csv('Especes_Tradescantia.csv')
Genera_Alocasia <- read.csv('Especes_Alocasia.csv')
Genera_Papaver <- read.csv('Especes_Papaver.csv')
Genera_Crotalaria <- read.csv('Especes_Crotalaria.csv')
Genera_Phyllanthus <- read.csv('Especes_Phyllanthus.csv')
Genera_Dendrobium <- read.csv('Especes_Dendrobium.csv')
Genera_Anthurium <- read.csv('Especes_Anthurium.csv')
Genera_Lamium <- read.csv('Especes_Lamium.csv')
Genera_Sedum$Rank<-rank( desc (Genera_Sedum$Images_Sedum))
Genera_Sedum <- read.csv('Especes_Sedum.csv')
install.packages("dplyr")
desc <- dplyr::desc
library(dplyr)
Genera_Acacia <- read.csv('Especes_Acacia.csv')
Genera_Trifolium <- read.csv('Especes_Trifolium.csv')
Genera_Hypericum <- read.csv('Especes_Hypericum.csv')
Genera_Ophrys <- read.csv('Especes_Ophrys.csv')
Genera_Anemone <- read.csv('Especes_Anemone.csv')
Genera_Cirsium <- read.csv('Especes_Cirsium.csv')
Genera_Pelargonium <- read.csv('Especes_Pelargonium.csv')
Genera_Peperomia <- read.csv('Especes_Peperomia.csv')
Genera_Lupinus <- read.csv('Especes_Lupinus.csv')
Genera_Dryopteris <- read.csv('Especes_Dryopteris.csv')
Genera_Lactuca <- read.csv('Especes_Lactuca.csv')
Genera_Tradescantia <- read.csv('Especes_Tradescantia.csv')
Genera_Alocasia <- read.csv('Especes_Alocasia.csv')
Genera_Papaver <- read.csv('Especes_Papaver.csv')
Genera_Crotalaria <- read.csv('Especes_Crotalaria.csv')
Genera_Phyllanthus <- read.csv('Especes_Phyllanthus.csv')
Genera_Dendrobium <- read.csv('Especes_Dendrobium.csv')
Genera_Anthurium <- read.csv('Especes_Anthurium.csv')
Genera_Lamium <- read.csv('Especes_Lamium.csv')
Genera_Sedum$Rank<-rank( desc (Genera_Sedum$Images_Sedum))
Genera_Acacia$Rank<-rank( desc (Genera_Acacia$Images_Acacia))
Genera_Trifolium$Rank<-rank( desc (Genera_Trifolium$Images_Trifolium))
Genera_Hypericum$Rank<-rank( desc (Genera_Hypericum$Images_Hypericum))
Genera_Ophrys$Rank<-rank( desc (Genera_Ophrys$Images_Ophrys))
Genera_Anemone$Rank<-rank( desc (Genera_Anemone$Images_Anemone))
Genera_Cirsium$Rank<-rank( desc (Genera_Cirsium$Images_Cirsium))
Genera_Pelargonium$Rank<-rank( desc (Genera_Pelargonium$Images_Pelargonium))
Genera_Peperomia$Rank<-rank( desc (Genera_Peperomia$Images_Peperomia))
Genera_Lupinus$Rank<-rank( desc (Genera_Lupinus$Images_Lupinus))
Genera_Dryopteris$Rank<-rank( desc (Genera_Dryopteris$Images_Dryopteris))
Genera_Lactuca$Rank<-rank( desc (Genera_Lactuca$Nombre_Images))
Genera_Tradescantia$Rank<-rank( desc (Genera_Tradescantia$Images_Tradescantia))
Genera_Alocasia$Rank<-rank( desc (Genera_Alocasia$Images_Alocasia))
Genera_Papaver$Rank<-rank( desc (Genera_Papaver$Images_Papaver))
Genera_Crotalaria$Rank<-rank( desc (Genera_Crotalaria$Images_Crotalaria))
Genera_Phyllanthus$Rank<-rank( desc (Genera_Phyllanthus$Images_Phyllanthus))
Genera_Dendrobium$Rank<-rank( desc (Genera_Dendrobium$Images_Dendrobium))
Genera_Anthurium$Rank<-rank( desc (Genera_Anthurium$Images_Anthurium))
Genera_Lamium$Rank<-rank( desc (Genera_Lamium$Images_Lamium))
Liste = list(Genera_Sedum$Rank, Genera_Acacia$Rank, Genera_Trifolium$Rank, Genera_Hypericum$Rank, Genera_Ophrys$Rank, Genera_Anemone$Rank, Genera_Cirsium$Rank, Genera_Pelargonium$Rank, Genera_Peperomia$Rank, Genera_Lupinus$Rank, Genera_Dryopteris$Rank, Genera_Lactuca$Rank, Genera_Tradescantia$Rank, Genera_Alocasia$Rank, Genera_Papaver$Rank, Genera_Crotalaria$Rank, Genera_Phyllanthus$Rank, Genera_Dendrobium$Rank, Genera_Anthurium$Rank, Genera_Lamium$Rank)
Liste2 = list(Genera_Sedum$Images_Sedum, Genera_Acacia$Images_Acacia, Genera_Trifolium$Images_Trifolium, Genera_Hypericum$Images_Hypericum, Genera_Ophrys$Images_Ophrys, Genera_Anemone$Images_Anemone, Genera_Cirsium$Images_Cirsium, Genera_Pelargonium$Images_Pelargonium, Genera_Peperomia$Images_Peperomia, Genera_Lupinus$Images_Lupinus, Genera_Dryopteris$Images_Dryopteris, Genera_Lactuca$Nombre_Images, Genera_Tradescantia$Images_Tradescantia, Genera_Alocasia$Images_Alocasia, Genera_Papaver$Images_Papaver, Genera_Crotalaria$Images_Crotalaria, Genera_Phyllanthus$Images_Phyllanthus, Genera_Dendrobium$Images_Dendrobium, Genera_Anthurium$Images_Anthurium, Genera_Lamium$Images_Lamium)
typeof(unlist(Liste[1]))
Genera_Sedum$Rank
typeof(unlist(Liste2[1]))
log(Genera_Sedum$Images_Sedum)
log(unlist(Liste2[1]))
Genera_Lamium
#R code
set.seed(8)
Liste3 = c()
Liste4 = c()
for (i in 1:20){
q.hats <- c()
beta.hats <- c()
ss.hats <- c()
for (bx in 1:10) {
q.mc <- c()
res.sq.mc <- c()
for (b in 1:30) {
q.b <- runif(1, 0, 100)
q.mc <- append(q.mc, q.b)
print(q.b)
print(length(q.b))
print(Liste)
print(length(unlist(Liste[i])))
res.sq.b <- sum( lm(log(unlist(Liste2[i])) ~ log(unlist(Liste[i]) +q.b))$residuals^2)
res.sq.mc <- append(res.sq.mc, res.sq.b)
}
res.q.mc.dat <- data.frame(q.mc,res.sq.mc)
q.hat <- res.q.mc.dat[which(res.q.mc.dat$res.sq.mc == min(res.q.mc.dat$res.sq.mc) ),]$q.mc
beta.hat <- lm(log(unlist(Liste2[i])) ~ log(unlist(Liste[i]) + q.hat))$coefficients[2]
ss.hat <- sum( lm(log(unlist(Liste2[i])) ~ log(unlist(Liste[i]) + q.hat))$residuals^2)
q.hats <- append(q.hats, q.hat)
beta.hats <- append(beta.hats, beta.hat)
ss.hats <- append(ss.hats, ss.hat)
qbeta.dat <- data.frame(q.hats,beta.hats)
}
fit <- lm( log(unlist(Liste2[i])) ~ log(unlist(Liste[i]) + mean(q.hats)))
Liste3[[(length(Liste3) + 1)]] <- fit
Liste4[[(length(Liste4) + 1)]] <- mean(q.hats)
}
nlm2 = nlm(f = negloglik3, p = c(2, 4), hessian = F, check.analyticals=TRUE, print.level=2)
nlm2 = nlm(f = negloglik3, p = c(2, 4), hessian = F, check.analyticals=F, print.level=2)
nlm2 = nlm(f = negloglik3, p = c(2, 4), hessian = F, check.analyticals=F, print.level=2, gradtol = 1e-12, steptol = 1e-9)
#Ensuite avec le gradient en plus
negloglik3 <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
res <- -sum(log(1/((x + parms[1])^(parms[2])*invC)))
tampon <- sum(-parms[2]/(1:N + parms[1])^(parms[2]+1))
tampon2 <- sum(exp(-parms[2]*log(1:N + parms[1])))
tampon3 <- sum(-log(1:N + parms[1])*exp(-parms[2]*log(1:N + parms[1])))
A <- sum((parms[2]/(x + parms[1])) + (tampon/invC))
B <- sum(log(x + parms[1]) + (tampon3/tampon2))
attr(res, "gradient") <- c(A, B)
res
}
gradient <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
tampon <- sum(-parms[2]/(1:N + parms[1])^(parms[2]+1))
tampon2 <- sum(exp(-parms[2]*log(1:N + parms[1])))
tampon3 <- sum(-log(1:N + parms[1])*exp(-parms[2]*log(1:N + parms[1])))
A <- sum((parms[2]/(x + parms[1])) + (tampon/invC))
B <- sum(log(x + parms[1]) + (tampon3/tampon2))
return(c(A, B))
}
#Ensuite avec le gradient en plus
negloglik3 <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
res <- -sum(log(1/((x + parms[1])^(parms[2])*invC)))
tampon <- sum(-parms[2]/(1:N + parms[1])^(parms[2]+1))
tampon2 <- sum(exp(-parms[2]*log(1:N + parms[1])))
tampon3 <- sum(-log(1:N + parms[1])*exp(-parms[2]*log(1:N + parms[1])))
A <- sum((parms[2]/(x + parms[1])) + (tampon/invC))
B <- sum(log(x + parms[1]) + (tampon3/tampon2))
attr(res, "gradient") <- c(A, B)
res
}
negloglik4 <- function(parms1, parms2) {
invC <- sum(1/(1:N + parms1)^parms2)
res <- -sum(log(1/((x + parms1)^(parms2)*invC)))
tampon <- sum(-parms2/(1:N + parms1)^(parms2+1))
tampon2 <- sum(exp(-parms2*log(1:N + parms1)))
tampon3 <- sum(-log(1:N + parms1)*exp(-parms2*log(1:N + parms1)))
A <- sum((parms2/(x + parms1)) + (tampon/invC))
B <- sum(log(x + parms1) + (tampon3/tampon2))
attr(res, "gradient") <- c(A, B)
res
}
#La fonction gradient
gradient <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
tampon <- sum(-parms[2]/(1:N + parms[1])^(parms[2]+1))
tampon2 <- sum(exp(-parms[2]*log(1:N + parms[1])))
tampon3 <- sum(-log(1:N + parms[1])*exp(-parms[2]*log(1:N + parms[1])))
A <- sum((parms[2]/(x + parms[1])) + (tampon/invC))
B <- sum(log(x + parms[1]) + (tampon3/tampon2))
return(c(A, B))
}
negloglik(parms = c(2, 4))
negloglik2(2, 4)
negloglik3(parms = c(2, 4))
negloglik4(2, 4)
gradient(c(2, 4))
optim = optim(par = c(2, 4), fn = negloglik, hessian = F)
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, hessian = F, method="BFGS")
optim
optim2
gradient(optim$par)
gradient(optim2$par)
nlm <- nlm(negloglik, p=c(20,20), gradtol = 1e-12, steptol = 1e-9)
nlm2 = nlm(f = negloglik3, p = c(2, 4), hessian = F, check.analyticals=F, print.level=2, gradtol = 1e-12, steptol = 1e-9)
#nlm3 = nlm(f = negloglik5, p = rep(2, 4), hessian = TRUE)
nlm
nlm2
gradient(nlm$estimate)
gradient(nlm2$estimate) #Cela ne fonctionne pas...
Pdf_Mandelbrot <- function(parms){
invC <- sum(1/(1:N + parms[1])^parms[2])
return(1/(x + parms[1])^(parms[2])*invC)
}
#On teste les fonctions
Pdf_Mandelbrot(c(2, 4))
#On teste les fonctions
plot(Pdf_Mandelbrot(c(2, 4)))
x
Genera_Sedum
x2=Genera_Sedum$Rank
x2
Pdf_Mandelbrot <- function(parms){
invC <- sum(1/(1:N + parms[1])^parms[2])
return(1/(x2 + parms[1])^(parms[2])*invC)
}
#On teste les fonctions
plot(Pdf_Mandelbrot(c(2, 4)))
#On teste les fonctions
plot(Pdf_Mandelbrot(c(56, -92)))
#On teste les fonctions
plot(Pdf_Mandelbrot(c(20, 20)))
#D'abord juste le negloglikelihood
negloglik <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
return(-sum(log(1/((x2 + parms[1])^(parms[2])*invC))))
}
negloglik2 <- function(parms1, parms2) {
invC <- sum(1/(1:N + parms1)^parms2)
return(-sum(log(1/((x2 + parms1)^(parms2)*invC))))
}
#Ensuite avec le gradient en plus
negloglik3 <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
res <- -sum(log(1/((x2 + parms[1])^(parms[2])*invC)))
tampon <- sum(-parms[2]/(1:N + parms[1])^(parms[2]+1))
tampon2 <- sum(exp(-parms[2]*log(1:N + parms[1])))
tampon3 <- sum(-log(1:N + parms[1])*exp(-parms[2]*log(1:N + parms[1])))
A <- sum((parms[2]/(x2 + parms[1])) + (tampon/invC))
B <- sum(log(x2 + parms[1]) + (tampon3/tampon2))
attr(res, "gradient") <- c(A, B)
res
}
negloglik4 <- function(parms1, parms2) {
invC <- sum(1/(1:N + parms1)^parms2)
res <- -sum(log(1/((x2 + parms1)^(parms2)*invC)))
tampon <- sum(-parms2/(1:N + parms1)^(parms2+1))
tampon2 <- sum(exp(-parms2*log(1:N + parms1)))
tampon3 <- sum(-log(1:N + parms1)*exp(-parms2*log(1:N + parms1)))
A <- sum((parms2/(x2 + parms1)) + (tampon/invC))
B <- sum(log(x2 + parms1) + (tampon3/tampon2))
attr(res, "gradient") <- c(A, B)
res
}
#La fonction gradient
gradient <- function(parms) {
invC <- sum(1/(1:N + parms[1])^parms[2])
tampon <- sum(-parms[2]/(1:N + parms[1])^(parms[2]+1))
tampon2 <- sum(exp(-parms[2]*log(1:N + parms[1])))
tampon3 <- sum(-log(1:N + parms[1])*exp(-parms[2]*log(1:N + parms[1])))
A <- sum((parms[2]/(x2 + parms[1])) + (tampon/invC))
B <- sum(log(x2 + parms[1]) + (tampon3/tampon2))
return(c(A, B))
}
negloglik(parms = c(2, 4))
negloglik2(2, 4)
negloglik3(parms = c(2, 4))
negloglik4(2, 4)
gradient(c(2, 4))
optim = optim(par = c(2, 4), fn = negloglik, hessian = F)
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, hessian = F, method="BFGS")
optim
optim2
gradient(optim$par)
gradient(optim2$par)
nlm <- nlm(negloglik, p=c(20,20), gradtol = 1e-12, steptol = 1e-9)
nlm2 = nlm(f = negloglik3, p = c(2, 4), hessian = F, check.analyticals=F, print.level=2, gradtol = 1e-12, steptol = 1e-9)
#nlm3 = nlm(f = negloglik5, p = rep(2, 4), hessian = TRUE)
nlm
nlm2
gradient(nlm$estimate)
gradient(nlm2$estimate)
optim = optim(par = c(2, 4), fn = negloglik, hessian = F)
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, hessian = F, method="BFGS")
optim
optim2
#Vérifions que le gradient est nul aux points optimaux
gradient(optim$par)
gradient(optim2$par)
nlm <- nlm(negloglik, p=c(20,20), gradtol = 1e-12, steptol = 1e-9)
nlm2 = nlm(f = negloglik3, p = c(2, 4), hessian = F, check.analyticals=F, print.level=2, gradtol = 1e-12, steptol = 1e-9)
#nlm3 = nlm(f = negloglik5, p = rep(2, 4), hessian = TRUE)
nlm
nlm2
nlm3
gradient(nlm$estimate)
gradient(nlm2$estimate)
#Essayons différentes méthodes d'optimisation avec mle
#D'abord la méthode par défaut, Nelder-Mead, qui ne donne rien
mle(negloglik2, start=list(parms1=2, parms2=4), method = "Nelder-Mead")
#Ensuite d'autres méthodes ne fonctionnant pas
mle(negloglik2, start=list(parms1=2, parms2=4), method = "BFGS")
#Ensuite d'autres méthodes ne fonctionnant pas
mle(negloglik2, start=list(parms1=2, parms2=4), method = "BFGS")
mle(negloglik2, start=list(parms1=2, parms2=4), method = "L-BFGS-B")
mle(negloglik2, start=list(parms1=2, parms2=4), gr = gradient, method = "SANN")
optim2
#Représentons une Mandelbrot avec les coeffcients obtenus
Pdf_Mandelbrot(optim2$par)
#Représentons une Mandelbrot avec les coeffcients obtenus
plot(Pdf_Mandelbrot(optim2$par))
plot(x, x2)
plot(x2, x)
sum(x)
x=Genera_Sedum$Images_Sedum
x
x/sum(x)
x
x=Genera_Sedum$Images_Sedum/sum(x)
x
x=Genera_Sedum$Images_Sedum/sum(x)
x
x
y=x/sum(x)
y
plot(x2, y)
#Représentons une Mandelbrot avec les coeffcients obtenus
plot(Pdf_Mandelbrot(optim2$par))
#Représentons graphiquement nos données
plot(x2, y)
#Représentons graphiquement nos données
plot(x2, y, type='l')
#Représentons une Mandelbrot avec les coeffcients obtenus
plot(Pdf_Mandelbrot(optim2$par))
Pdf_Mandelbrot <- function(parms){
invC <- sum(1/(1:N + parms[1])^parms[2])
return(1/(x2 + parms[1])^(parms[2])*(1/invC))
}
#Représentons une Mandelbrot avec les coeffcients obtenus
plot(Pdf_Mandelbrot(optim2$par))
optim2
(1 / (1 + 5.403)^(-0.0001))
sum(1/(1:N + 5.403)^(-0.0001))
invC = sum(1/(1:N + 5.403)^(-0.0001))
invC
(1 / (1 + 5.403)^(-0.0001))*invC
(1 / (10 + 5.403)^(-0.0001))*invC
help(nlm)
optim = optim(par = c(2, 4), fn = negloglik, hessian = F)
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, hessian = F, method="BFGS")
optim
optim2
help(optim)
optim
#Let's use different general optimizers of R
#D'abord optim, où l'on peut préciser le gradient avec gr=...
#d'abord sans préciser le gradient, puis ensuite en le précisant
optim = optim(par = c(2, 4), fn = negloglik, lower = c(0,0) hessian = F)
#Let's use different general optimizers of R
#D'abord optim, où l'on peut préciser le gradient avec gr=...
#d'abord sans préciser le gradient, puis ensuite en le précisant
optim = optim(par = c(2, 4), fn = negloglik, lower = c(0,0), hessian = F)
#Let's use different general optimizers of R
#D'abord optim, où l'on peut préciser le gradient avec gr=...
#d'abord sans préciser le gradient, puis ensuite en le précisant
optim = optim(par = c(2, 4), fn = negloglik, lower = c(0,0), hessian = F, method='L-BFGS-B')
optim
#Let's use different general optimizers of R
#D'abord optim, où l'on peut préciser le gradient avec gr=...
#d'abord sans préciser le gradient, puis ensuite en le précisant
optim = optim(par = c(2, 4), fn = negloglik, lower = c(0,1e-12), hessian = F, method='L-BFGS-B')
optim
#Vérifions que le gradient est nul aux points optimaux
gradient(optim$par)
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, lower = c(0, 1e-12), hessian = F, method="BFGS")
optim2 = optim(par = c(2, 4), fn = negloglik, gr = gradient, lower = c(0, 1e-12), hessian = F, method="L-BFGS-B")
optim2
#Représentons une Mandelbrot avec les coeffcients obtenus
plot(Pdf_Mandelbrot(optim$par))
#Représentons graphiquement nos données
plot(x2, y, type='l')
plot(Pdf_Mandelbrot(optim$par), col='blue')
lines(x2, y, type='l', col='red')
plot(Pdf_Mandelbrot(optim$par), ylim=c(0, max(y)), col='blue')
lines(x2, y, type='l', col='red')
#Let's use different general optimizers of R
#D'abord optim, où l'on peut préciser le gradient avec gr=...
#d'abord sans préciser le gradient, puis ensuite en le précisant
optim = optim(par = c(2, 4), fn = negloglik, lower = c(0,10), hessian = F, method='L-BFGS-B')
optim
plot(Pdf_Mandelbrot(optim$par), ylim=c(0, max(y)), col='blue')
lines(x2, y, type='l', col='red')
x2
