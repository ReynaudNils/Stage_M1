Estimateur_N = (125*155) / (21)
Estimateur_N
N = 923
pi = 0.15
Estimateur_N = c()
for (j in 1:100){
echantillon = Modèle_M(N, pi)
while (echantillon[3] == 0) {echantillon = Modèle_M(N, pi)} ##Dans l'estimateur N  il faut diviser par $C_{21}$ donc il faut s'assurer que ce terme est non nul
Estimateur_N = c(Estimateur_N, echantillon[1] * ((echantillon[2] + echantillon[3]) / (echantillon[3])))
}
N = 923
pi = 0.15
Estimateur_N = c()
for (j in 1:100){
echantillon = Modele_M(N, pi)
while (echantillon[3] == 0) {echantillon = Modele_M(N, pi)} ##Dans l'estimateur N  il faut diviser par $C_{21}$ donc il faut s'assurer que ce terme est non nul
Estimateur_N = c(Estimateur_N, echantillon[1] * ((echantillon[2] + echantillon[3]) / (echantillon[3])))
}
N = 923
Estimateur_N = c()
for (j in 1:100){
echantillon = Modele_M(N, 0.15)
while (echantillon[3] == 0) {echantillon = Modele_M(N, 0.15)} ##Dans l'estimateur N  il faut diviser par $C_{21}$ donc il faut s'assurer que ce terme est non nul
Estimateur_N = c(Estimateur_N, echantillon[1] * ((echantillon[2] + echantillon[3]) / (echantillon[3])))
}
my_rbinomiale <- function(n, N, pi){
U=runif(n)
F_k=pbinom(0:N, size=N, prob=pi)
return(findInterval(U, F_k))
}
n = 1000
N = 125
pi = 0.15
freq_theo = dbinom(0:N, N, pi)
C1 <- my_rbinomiale(n, N, pi)
resultats <- data.frame(n=1:N, valeurs=factor(C1, levels = 0:N))
freq_emp <- c()
for (k in 0:N){
freq_emp <- c(freq_emp, mean(C1==k))
}
freq_binomiale <- tibble( x=0:N, freq_emp=freq_emp, freq_theo=freq_theo)
# Représentation graphique
ggplot(freq_binomiale) +
aes(x = x) +
geom_col(mapping = aes(y = freq_emp),
width = 0.2, fill = "blue") +
geom_point(aes(y = freq_theo),
shape = 3, col = "red", size = 3) +
labs(y = "Comparaison_Frequence", x = "Nombre de succès")
Modele_M <- function(N, pi){
C1 <- my_rbinomiale(1, N, pi)
C20 <- my_rbinomiale(1, N-C1, pi)
C21 <- my_rbinomiale(1, C1, pi)
return(c(C1,C20,C21))
}
Estimateur_pi = (125 + 134 + 21) / (2*950)
Estimateur_pi
x_beta <- seq(0, 1, length.out=500)
A_prior_beta <- dbeta(x_beta, shape1 = 1, shape2 = 3)
A_posterior_beta <- dbeta(x_beta, shape1 = 281, shape2 = 1623)
plot(x_beta, A_prior_beta, col='red', type='l')
lines(x_beta, A_posterior_beta, col='blue')
abline(v=(125+134+21)/(2*950), col = 3)
Estimateur_N = (125*155) / (21)
Estimateur_N
N = 923
Estimateur_N = c()
for (j in 1:100){
echantillon = Modele_M(N, 0.15)
while (echantillon[3] == 0) {echantillon = Modele_M(N, 0.15)} ##Dans l'estimateur N  il faut diviser par $C_{21}$ donc il faut s'assurer que ce terme est non nul
Estimateur_N = c(Estimateur_N, echantillon[1] * ((echantillon[2] + echantillon[3]) / (echantillon[3])))
}
Biais_relatif = (mean(Estimateur_N) - N)/N
Biais_relatif
N = seq(100, 1000, 10)
Biais_relatif = c()
for (k in N){
Estimateur_N = c()
for (j in 1:100){
echantillon = Modele_M(N, 0.15)
while (echantillon[3] == 0) {
echantillon = Modele_M(N, 0.15)} ##Dans l'estimateur N il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, (echantillon[1] * (echantillon[2] + echantillon[3])/(echantillon[3])))
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - k)/k)
}
N = seq(100, 1000, 10)
biais_relatif = c()
for(i in N){
estim_N = c()
for(k in 1:100){
gener = Modele_M(i, 0.15) ##on génère
while(gener[[3]] == 0){ gener[3] = Modele_M(i, 0.15)[3]}
##comme on divise par C_{21} on est obligé de s'assurer que gener[3]=C_{21} !=0
estim_N = c(estim_N, gener[[1]]*(gener[[2]]+gener[[3]])/gener[[3]]) #C_{2}=gener[2]+gener[3]
}
biais_relatif = c(biais_relatif, (mean(estim_N) - i)/i)
}
plot(N, biais_relatif)
N = seq(100, 1000, 10)
Biais_relatif = c()
for (k in N){
Estimateur_N = c()
for (j in 1:100){
echantillon = Modele_M(N, 0.15)
while (echantillon[3] == 0) {
echantillon [3] = Modele_M(N, 0.15)[3]} ##Dans l'estimateur N il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, (echantillon[1] * (echantillon[2] + echantillon[3])/(echantillon[3])))
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - k)/k)
}
N = seq(100, 1000, 10)
Biais_relatif = c()
for (k in N){
Estimateur_N = c()
for (j in 1:100){
echantillon = Modele_M(k, 0.15)
while (echantillon[3] == 0) {
echantillon = Modele_M(N, 0.15)} ##Dans l'estimateur N il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, (echantillon[1] * (echantillon[2] + echantillon[3])/(echantillon[3])))
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - k)/k)
}
N = seq(100, 1000, 10)
Biais_relatif = c()
for (i in N){
Estimateur_N = c()
for (k in 1:100){
echantillon = Modele_M(i, 0.15)
while (echantillon[3] == 0) {
echantillon = Modele_M(N, 0.15)} ##Dans l'estimateur N il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, (echantillon[1] * (echantillon[2] + echantillon[3])/(echantillon[3])))
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - k)/k)
}
N = seq(100, 1000, 10)
Biais_relatif = c()
for (i in N){
Estimateur_N = c()
for (k in 1:100){
echantillon = Modele_M(i, 0.15)
while (echantillon[[3]] == 0) {
echantillon [3] = Modele_M(N, 0.15)[3]} ##Dans l'estimateur N il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, (echantillon[[1]] * (echantillon[[2]] + echantillon[[3]])/(echantillon[[3]])))
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - k)/k)
}
N = seq(100, 1000, 10)
Biais_relatif = c()
for(i in N){
Estimateur_N = c()
for(k in 1:100){
gener = Modele_M(i, 0.15) ##on génère
while(gener[[3]] == 0){ gener[3] = Modele_M(i, 0.15)[3]}
##comme on divise par C_{21} on est obligé de s'assurer que gener[3]=C_{21} !=0
Estimateur_N = c(Estimateur_N, gener[[1]]*(gener[[2]]+gener[[3]])/gener[[3]]) #C_{2}=gener[2]+gener[3]
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - i)/i)
}
plot(N, Biais_relatif)
N = seq(100, 1000, 10)
Biais_relatif = c()
for (i in N){
Estimateur_N = c()
for (k in 1:100){
echantillon = Modele_M(i, 0.15)
while (echantillon[[3]] == 0) {
echantillon [3] = Modele_M(i, 0.15)[3]} ##Dans l'estimateur N il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, (echantillon[[1]] * (echantillon[[2]] + echantillon[[3]])/(echantillon[[3]])))
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - k)/k)
}
plot(N, Biais_relatif)
N = seq(100, 1000, 10)
Biais_relatif = c()
for (i in N){
Estimateur_N = c()
for (k in 1:100){
echantillon = Modele_M(i, 0.15)
while (echantillon[[3]] == 0) {
echantillon [3] = Modele_M(i, 0.15)[3]} ##Dans l'estimateur N il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, (echantillon[[1]] * (echantillon[[2]] + echantillon[[3]])/(echantillon[[3]])))
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - i)/i)
}
plot(N, Biais_relatif)
N = seq(100, 1000, 10)
Biais_relatif = c()
for (i in N){
Estimateur_N = c()
for (k in 1:100){
echantillon = Modele_M(i, 0.15)
while (echantillon[[3]] == 0) {
echantillon = Modele_M(i, 0.15)} ##Dans l'estimateur N il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, (echantillon[[1]] * (echantillon[[2]] + echantillon[[3]])/(echantillon[[3]])))
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - i)/i)
}
plot(N, Biais_relatif)
N = seq(100, 1000, 10)
Biais_relatif = c()
for (j in N){
Estimateur_N = c()
for (k in 1:100){
echantillon = Modele_M(j, 0.15)
while (echantillon[[3]] == 0) {
echantillon = Modele_M(j, 0.15)} ##Dans l'estimateur N il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, (echantillon[[1]] * (echantillon[[2]] + echantillon[[3]])/(echantillon[[3]])))
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - j)/j)
}
plot(N, Biais_relatif)
N = seq(100, 1000, 10)
Biais_relatif = c()
for (j in N){
Estimateur_N = c()
for (k in 1:100){
echantillon = Modele_M(j, 0.15)
while (echantillon[3] == 0) {
echantillon = Modele_M(j, 0.15)} ##Dans l'estimateur N il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, (echantillon[1] * (echantillon[2] + echantillon[3])/(echantillon[3])))
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - j)/j)
}
plot(N, Biais_relatif)
N = 923
Estimateur_N = c()
for (j in 1:100){
echantillon = Modele_M(N, 0.15)
while (echantillon[3] == 0) {echantillon = Modele_M(N, 0.15)} ##Dans l'estimateur N  il faut diviser par $C_{21}$ donc il faut s'assurer que ce terme est non nul
Estimateur_N = c(Estimateur_N, echantillon[1] * ((echantillon[2] + echantillon[3]) / (echantillon[3])))
}
Biais_relatif = (mean(Estimateur_N) - N)/N
Biais_relatif
N = 923
Estimateur_N = c()
for (j in 1:100){
echantillon = Modele_M(N, 0.15)
while (echantillon[3] == 0) {echantillon = Modele_M(N, 0.15)} ##Dans l'estimateur N  il faut diviser par $C_{21}$ donc il faut s'assurer que ce terme est non nul
Estimateur_N = c(Estimateur_N, echantillon[1] * ((echantillon[2] + echantillon[3]) / (echantillon[3])))
}
Biais_relatif = (mean(Estimateur_N) - N)/N
Biais_relatif
N = seq(100, 1000, 10)
Biais_relatif = c()
for (j in N){
Estimateur_N = c()
for (k in 1:100){
echantillon = Modele_M(j, 0.15)
while (echantillon[3] == 0) {
echantillon = Modele_M(j, 0.15)} ##Dans l'estimateur N il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, (echantillon[1] * (echantillon[2] + echantillon[3])/(echantillon[3])))
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - j)/j)
}
plot(N, Biais_relatif)
`
N = seq(100, 1000, 10)
Biais_relatif = c()
for (j in N){
Estimateur_N = c()
for (k in 1:100){
echantillon = Modele_M(j, 0.15)
while (echantillon[3] == 0) {
echantillon = Modele_M(j, 0.15)} ##Dans l'estimateur N il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, (echantillon[1] * (echantillon[2] + echantillon[3])/(echantillon[3])))
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - j)/j)
}
plot(N, Biais_relatif)
N = seq(100, 1000, 10)
Biais_relatif = c()
for (j in N){
Estimateur_N = c()
for (k in 1:100){
echantillon = Modele_M(j, 0.15)
while (echantillon[3] == 0) {
echantillon = Modele_M(j, 0.15)} ##Dans l'estimateur N il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, (echantillon[1] * (echantillon[2] + echantillon[3])/(echantillon[3])))
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - j)/j)
}
plot(N, Biais_relatif)
N = 923
Estimateur_N = c()
for (k in 1:100){
echantillon = Modele_M(N, 0.15)
while (echantillon[3] == 0) {echantillon = Modele_M(N, 0.15)} ##Dans l'estimateur N  il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, echantillon[1] * ((echantillon[2] + echantillon[3]) / (echantillon[3])))
}
Biais_relatif = (mean(Estimateur_N) - N)/N
Biais_relatif
N = seq(100, 1000, 10)
Biais_relatif = c()
for (j in N){
Estimateur_N = c()
for (k in 1:100){
echantillon = Modele_M(j, 0.15)
while (echantillon[3] == 0) {
echantillon = Modele_M(j, 0.15)} ##Dans l'estimateur N il faut diviser par $C_{21}$ donc faire en sorte que ce terme soit non nul
Estimateur_N = c(Estimateur_N, (echantillon[1] * (echantillon[2] + echantillon[3])/(echantillon[3])))
}
Biais_relatif = c(Biais_relatif, (mean(Estimateur_N) - j)/j)
}
plot(N, Biais_relatif)
library(zipfR)
library(zipfR)
ZM = Inre("zm", alpha = 2/3, B = 0.1)
zmsample = rInre(ZM, n=100)
help(Inre)
??Inre
??rInre
ZM = Inre("zm", alpha = 2/3, B = 0.1)
ZM = lnre("zm", alpha = 2/3, B = 0.1)
zmsample = rlnre(ZM, n=100)
zmnumeric = as.numeric(as.character(zmsample))
plot(zmnumeric)
library(zipfR)
ZM = lnre("zm", alpha = 1.5, B = 0.1)
zmsample = rlnre(ZM, n=100)
zmnumeric = as.numeric(as.character(zmsample))
plot(zmnumeric)
library(zipfR)
ZM = lnre("zm", alpha = 1.5, B = 0.1)
zmsample = rlnre(ZM, n=1000)
zmnumeric = as.numeric(as.character(zmsample))
plot(zmnumeric)
zmsample = rlnre(ZM, n=500)
library(zipfR)
ZM = lnre("zm", alpha = 1.5, B = 0.1)
zmsample = rlnre(ZM, n=500)
zmnumeric = as.numeric(as.character(zmsample))
plot(zmnumeric)
library(zipfR)
ZM = lnre("zm", alpha = 1.5, B = 0.1)
zmsample = rlnre(ZM, n=100)
zmnumeric = as.numeric(as.character(zmsample))
plot(zmnumeric)
library(ggplot2)
library(dplyr)
library(themes)
library(gganimate)
word_count <- # Data frame containing words and their frequency
colnames(word_count) <- c("word", "count")
alpha <- 1 # Change it needed
word_count <- word_count %>%
mutate(word = factor(word, levels = word),
rank = row_number(),
zipfs_freq = ifelse(rank == 1, count, dplyr::first(count) / rank^alpha))
zipfs_plot <- ggplot(word_count, aes(x = rank, y = count)) +
geom_point(aes(color = "observed")) +
theme_bw() +
geom_point(aes(y = zipfs_freq, color = "theoretical")) +
transition_reveal(count, rank) +
labs(x = "rank", y = "count", title = "Zipf's law visualization") +
scale_colour_manual(name = "Word count", values=c("theoretical" = "red", "observed" = "black")) +
theme(legend.position = "top")
zipfs_animation <- animate(p)
data(BCI)
data(meaudret)
vegan
library(vegan)
plot(1:10810)
source("C:/Users/nilsr/Desktop/PRE/ZM-Fitting.R")
library(stats4)
getwd()
setwd('C:/Users/nilsr/Desktop/PRE/Stage_M1/Jupyter_Notebook/Donnees_CSV')
install.packages("dplyr")
desc <- dplyr::desc
library(dplyr)
Data <- read.csv("Especes.csv")
Data
Data$Rank<-rank( desc (Data$Images_Plantes))
Data$Rank<-rank( desc (Data$Images_Plante))
Data
fr <- unlist(Data['Images_Plante'])
p <- fr/sum(fr)
typeof(p)
len <- length(p)
len
rank <- unlist(Data['Rank'])
rank
loglikzipf <- function(s,N) -s*log(rank)-log(sum(1/(1:N)^s))
#Sum of Squares
opt.f <- function(s) sum((log(p)-loglikzipf(s,length(p)))^2)
opt <- optimize(opt.f, c(0, 10))
#Maximum likelihood estimation
negloglikzipf <- function(s) sum(fr*(s*log(rank)+log(sum(1/(1:len)^s))))
fit <- mle(negloglikzipf,start=list(s=1))
s.sq <- opt$minimum
s.ll <- coef(fit)
opt
fit
plot(1:len,p, xlab="Generas", ylab="Frequencies")
lines(1:len,exp(loglikzipf(opt$minimum,len)),col="red")
lines(1:len,exp(loglikzipf(coef(fit),len)),col="blue")
title(main = "Fitting Zipf for Generas")
legend(x = "topright", legend = c("Moindre Carrés", "Max Vraisemblance"), col = c("red", "blue"), lty = 1, cex = 0.8)
library(stats4)
getwd()
setwd('C:/Users/nilsr/Desktop/PRE/Stage_M1/Jupyter_Notebook/Donnees_CSV')
#install.packages("dplyr")
#desc <- dplyr::desc
#library(dplyr)
Data <- read.csv("Dataset_Especes.csv")
Data
Data$Rank<-rank( desc (Data$Images_Classes))
Data
fr <- unlist(Data['Images_Classes'])
p <- fr/sum(fr)
typeof(p)
len <- length(p)
len
rank <- unlist(Data['Rank'])
rank
Data
Data <- read.csv("Dataset_Especes.csv")
Data
Data$Rank<-rank( desc (Data$Images_Classes))
Data
fr <- unlist(Data['Images_Classes'])
p <- fr/sum(fr)
typeof(p)
len <- length(p)
len
rank <- unlist(Data['Rank'])
rank
loglikzipf <- function(s,N) -s*log(rank)-log(sum(1/(1:N)^s))
#Sum of Squares
opt.f <- function(s) sum((log(p)-loglikzipf(s,length(p)))^2)
opt <- optimize(opt.f, c(0, 10))
#Maximum likelihood estimation
negloglikzipf <- function(s) sum(fr*(s*log(rank)+log(sum(1/(1:len)^s))))
fit <- mle(negloglikzipf,start=list(s=1))
s.sq <- opt$minimum
s.ll <- coef(fit)
opt
fit
plot(1:len,p, xlab="Generas", ylab="Frequencies")
lines(1:len,exp(loglikzipf(opt$minimum,len)),col="red")
lines(1:len,exp(loglikzipf(coef(fit),len)),col="blue")
title(main = "Fitting Zipf for Generas")
legend(x = "topright", legend = c("Moindre Carrés", "Max Vraisemblance"), col = c("red", "blue"), lty = 1, cex = 0.8)
chisq.test(p, exp(loglikzipf(opt$minimum,length(p))))
chisq.test(p, exp(loglikzipf(coef(fit),length(p))))
library(stats4)
getwd()
setwd('C:/Users/nilsr/Desktop/PRE/Stage_M1/Jupyter_Notebook/Donnees_CSV')
#install.packages("dplyr")
#desc <- dplyr::desc
#library(dplyr)
Data <- read.csv("Especes.csv")
Data
Data$Rank<-rank( desc (Data$Images_Plantes))
library(stats4)
getwd()
setwd('C:/Users/nilsr/Desktop/PRE/Stage_M1/Jupyter_Notebook/Donnees_CSV')
#install.packages("dplyr")
#desc <- dplyr::desc
#library(dplyr)
Data <- read.csv("Especes.csv")
Data
Data$Rank<-rank( desc (Data$Images_Plante))
Data
fr <- unlist(Data['Images_Plante'])
p <- fr/sum(fr)
typeof(p)
len <- length(p)
typeof(fr)
rank <- unlist(Data['Rank'])
rank
loglikzipf <- function(parms,N) -parms[2]*log(rank + parms[1])-log(sum(1/(1:N + parms[1])^parms[2]))
#Sum of Squares
opt.f <- function(parms) sum((log(p)-loglikzipf(parms,length(p)))^2)
opt <- optim(par = c(2, 4), opt.f, lower = c(0,0), method = "L-BFGS-B")
negloglikzipf <- function(parms1, parms2) sum(fr*(parms2*log(rank +parms1)+log(sum(1/(1:len + parms1)^parms2))))
fit <- mle(negloglikzipf,start=list(parms1=2, parms2=4))
opt$par
coef(fit)
library(stats4)
getwd()
setwd('C:/Users/nilsr/Desktop/PRE/Stage_M1/Jupyter_Notebook/Donnees_CSV')
#install.packages("dplyr")
#desc <- dplyr::desc
#library(dplyr)
Data <- read.csv("Dataset_Especes_ZM.csv")
Data
Data$Rank<-rank( desc (Data$Images_Classes))
Data
fr <- unlist(Data['Images_Classes'])
p <- fr/sum(fr)
typeof(p)
len <- length(p)
typeof(fr)
rank <- unlist(Data['Rank'])
rank
loglikzipf <- function(parms,N) -parms[2]*log(rank + parms[1])-log(sum(1/(1:N + parms[1])^parms[2]))
#Sum of Squares
opt.f <- function(parms) sum((log(p)-loglikzipf(parms,length(p)))^2)
opt <- optim(par = c(2, 4), opt.f, lower = c(0,0), method = "L-BFGS-B")
negloglikzipf <- function(parms1, parms2) sum(fr*(parms2*log(rank +parms1)+log(sum(1/(1:len + parms1)^parms2))))
fit <- mle(negloglikzipf,start=list(parms1=2, parms2=4))
coef(fit)
plot(1:len,p, xlab="Species of Genera Lupinus", ylab="Frequencies")
plot(1:len,p, xlab="Species of Genera Lupinus", ylab="Frequencies")
#lines(1:len,exp(loglikzipf(opt$par,len)),col="red")
lines(1:len,exp(loglikzipf(coef(fit),len)),p,col="blue")
plot(1:len,p, xlab="Species of Genera Lupinus", ylab="Frequencies")
#lines(1:len,exp(loglikzipf(opt$par,len)),col="red")
lines(1:len,exp(loglikzipf(coef(fit),len)),col="blue")
#lines(1:len,exp(loglikzipf(c(q2,z2),len)),col="green")
title(main = "Fitting Zipf-Mandelbrot for Genera Lupinus")
